{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis \n",
    "\n",
    "**Source signal**: $Z'$ vector boson. $M_Z'$  = __500 $GeV$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data analysis is divided on two stages:\n",
    "\n",
    "1. **Initial Cuts $C_0$** : Following uniandes HEP group research, it were made several initial cuts $C_0$ in order to reduce part of QCD background. These cuts were: \n",
    "    - $N_{jets} > 1$  and $N_{\\tau} > 1$\n",
    "    - $Q^1_{\\tau}*Q^2_{\\tau} < 0$\n",
    "    - $\\eta^1_{jets}*\\eta^2_{jets} < 0$\n",
    "    \n",
    "*Note* : it were selected the $\\tau$ and $jets$ with the highest $p_T$ \n",
    "\n",
    "\n",
    "These cuts were applied over the *signal_0p5TeV*, *DY+jets* and *W+jets* samples. From those cuts we get a sample called full_sample (it is not the full original simulated sample).\n",
    "\n",
    "2. **MVA**: Finally, to implement the MVA analysis, we decided to take a random sample of the signal and each background type, that then it will be merged in a *joined_* sample that have a balanced amount of data between signal and background, i.e. the *joined_* sample have $50 \\%$ of signal sample and $25 \\%$ of *DY+jets* background and $25 \\%$ of *W+jets* background.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set folder configuration \n",
    "folder_random = 'randomSample/'\n",
    "folder_full = 'fullSample/'\n",
    "signal_filename = 'signal_0p5TeV.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import random sample from ROOT\n",
    "signal = pd.read_csv(folder_random+'joined_'+signal_filename+'.csv')\n",
    "signal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some descriptive statistics from raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which variables we have\n",
    "signal.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first approach to new data, it's useful to see how their variables are distributed. Futhermore, as a *supervised learning problem*, we know the target varaible, so lets use it:\n",
    "\n",
    "*These graphs were generated with ROOT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('./'+folder_random+'histograms_'+signal_filename+'ByType_1.pdf',width=850,height=560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IFrame('./'+folder_random+'histograms_'+signal_filename+'ByType_2.pdf',width=850,height=560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IFrame('./'+folder_random+'histograms_'+signal_filename+'ByType_3.pdf',width=850,height=560)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous images, we can see that some variables could have higher predictable power to discriminate between **signal** and **background** if we take the absolute value of them:\n",
    "     \n",
    "    - Tau_Mass_dif, Jet_Mass_dif\n",
    "    - Jet_PT_dif, Tau_PT_dif\n",
    "    - Jet_Eta_dif, Jet_Eta_1, Jet_Eta_1\n",
    "    - Tau_Eta_dif, Tau_Eta_1, Tau_Eta_2    \n",
    "    - Tau_Phi_dif, Tau_Phi_1, Tau_Phi_2\n",
    "    - Jet_Phi_dif, Jet_Phi_1, Jet_Phi_2\n",
    "\n",
    "However, some of them (and others) seems don't have enough classification power:\n",
    "   \n",
    "    - Jet_PT_dif, Tau_PT_dif  \n",
    "    - Tau_Mass_dif, Jet_Mass_dif\n",
    "    - Tau_Eta_dif, Tau_Eta_1, Tau_Eta_2\n",
    "    - Tau_Phi_dif, Tau_Phi_1, Tau_Phi_2\n",
    "    - Miss_Eta, Miss_Phi\n",
    "    - Jet_Phi_dif, Jet_Phi_1, Jet_Phi_2\n",
    "    \n",
    "For now, let's take the absolute value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal[['Tau_Mass_dif','Jet_Mass_dif']] = signal[['Tau_Mass_dif','Jet_Mass_dif']].abs()\n",
    "signal[['Jet_PT_dif','Tau_PT_dif']] = signal[['Jet_PT_dif','Tau_PT_dif']].abs()\n",
    "signal[['Jet_Eta_dif','Jet_Eta_1','Jet_Eta_1']] = signal[['Jet_Eta_dif','Jet_Eta_1','Jet_Eta_1']].abs()\n",
    "signal[['Tau_Eta_dif','Tau_Eta_1','Tau_Eta_2']] = signal[['Tau_Eta_dif','Tau_Eta_1','Tau_Eta_2']].abs()\n",
    "signal[['Tau_Phi_dif','Tau_Phi_1','Tau_Phi_2']] = signal[['Tau_Phi_dif','Tau_Phi_1','Tau_Phi_2']].abs()\n",
    "signal[['Jet_Phi_dif','Jet_Phi_1','Jet_Phi_2']] = signal[['Jet_Phi_dif','Jet_Phi_1','Jet_Phi_2']].abs()\n",
    "signal[['Jet_Mass_sum']] = signal[['Jet_Mass_sum']].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide the data between dependent and independent variables\n",
    "Xsignal = signal.drop(['type'],axis=1)\n",
    "Ysignal = signal['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploraty Analysis\n",
    "\n",
    "### Pre-Pre-Processing \n",
    "\n",
    "The objective of the following sections is to explorate our data and find useful insigths that could gives us a flavor of the sense of the data. As the previous graphs shows, there are some variables that possiblely would not have importance in the prediction of the target variable - **type** of event = (**Signal**, **Background**) -, so the posterior analysis should confirm this of them. \n",
    "\n",
    "This process called *feature selection* is fundamental for the success of the model implementation. However, to avoid bias of *feature selection* we must divide the data in two parts: trainning-test set and validation set. To read more about the bias on *feature selection* see: http://www.pnas.org/content/99/10/6562.full.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train_test, X_validation, Y_train_test, Y_validation = train_test_split(Xsignal,Ysignal,test_size=.2)\n",
    "train_test = pd.concat([Y_train_test,X_train_test],axis=1)\n",
    "X_validation.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set will be usufull at the end of the model implementation to check for bias selection.\n",
    "\n",
    "\n",
    "\n",
    "### Correlation Matrix\n",
    "\n",
    "\n",
    "In order to look for the most important variables to classify each event between __signal__ and __Background__ we can calculate the matrix of correlations of the whole data set. We expect that higher correlations could have higher predictable power. As a side effect of this tool we can look for the presence of *multicolinearity* in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrmat = train_test.corr()\n",
    "f,ax =  plt.subplots(figsize=(12,9))\n",
    "sb.heatmap(corrmat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous **heatmap**, first we can see that **Tau_Eta/Phi_1/2/dif** variables almost don't have any correlation with the other variables. \n",
    "Second, according to the *Vector Boson Fusion* Topology the **Jet_Eta_1/2/dif** variables should have great discrimination power, i.e. high correlation with type. However, it seems that these aren't the best ones at this stage (**Note: Jet_Eta_dif/1/2** have a great importance in the background clearing step, previous to this analysis). In contrast, other variables related with **Tranverse Momentum** could have better fit. \n",
    "\n",
    "Finally, as a result of the merging information process, there are a lot of group of variables that are highly correlated between them, e.g. **Jet_PT_sum/dif/1/2**, because they represent the same physical observable. \n",
    "\n",
    "Let's look more in deeper to check which are the most important features (high correlation with **type**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correlation matrix ordered by correlation with type\n",
    "\n",
    "k = 20\n",
    "f,ax =  plt.subplots(figsize=(12,9))\n",
    "cols = corrmat.nlargest(k, 'type')['type'].index\n",
    "cm = np.corrcoef(train_test[cols].values.T)\n",
    "heatmap = sb.heatmap(cm,annot=True,fmt='.2f',yticklabels=cols.values,xticklabels=cols.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the previous ranking and avoiding *multicolinearity*, in each group of variables we can select the one with highest correlation with **type**:\n",
    "    1. Jet_Ener_sum\n",
    "    2. Jet_PT_sum\n",
    "    3. Tau_PT_sum\n",
    "    4. Jet_Mass_sum\n",
    "    5. Miss_MET\n",
    "    6. Tau_Ener_sum\n",
    "    7. Jet_Eta_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_test_selected = train_test[['Jet_Ener_sum','Jet_PT_sum','Tau_PT_sum','Jet_Mass_sum','Miss_MET','Tau_Ener_sum','Jet_Eta_dif']]\n",
    "Y_train_test_selected = train_test['type']\n",
    "train_test_selected = pd.concat([Y_train_test_selected,X_train_test_selected],axis=1)\n",
    "train_test_selected.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separabilty of Data\n",
    "\n",
    "To get a better approach to the data and how it is compound, it's possible to make a scatter plot across all variables. The idea is to know how would be the best function that can divide the data between **signal** and **background** events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb.pairplot(train_test_selected,hue='type',diag_kind='kde',palette='husl', markers='+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows that in some pair of variables it doesn't exist a linear function that can easily divide the data. Futhermore, as it's shown in the *Kernel distributions* plots over the diagonal, the background data is highly concentrated in some ranges. So to smoothe the variance of the data set over *type* of event, let's *log-transformated* the data (independent variables) in other to get a more comparable data by **type**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb.pairplot(pd.concat([Y_train_test_selected,log(X_train_test_selected)],axis=1),hue='type',diag_kind='kde',palette='husl',markers='+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach we can see a more clear separation than the previous figure, even it's possible to think that it would exist a linear function that can divide the data between **signal** and **background**\n",
    "\n",
    "To confirm how have changed the data, let's see the empirical distributions before and after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeJoinHist(dataframe,varname):  \n",
    "    leg =['1','0']\n",
    "    fig, ax = plt.subplots()\n",
    "    sb.distplot(dataframe[dataframe['type']==1][varname], ax = ax, label=r\"$Z'\\rightarrow \\tau \\tau$\",color='b')\n",
    "    sb.distplot(dataframe[dataframe['type']==0][varname], ax = ax, label='SM Background',color='r')\n",
    "    fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeJoinHist(train_test_selected,'Jet_Ener_sum')\n",
    "plt.title(\"Original\")\n",
    "makeJoinHist(pd.concat([Y_train_test_selected,log(X_train_test_selected)],axis=1),'Jet_Ener_sum')\n",
    "plt.title(\"Log-transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeJoinHist(train_test_selected,'Jet_PT_sum')\n",
    "plt.title(\"Original\")\n",
    "makeJoinHist(pd.concat([Y_train_test_selected,log(X_train_test_selected)],axis=1),'Jet_PT_sum')\n",
    "plt.title(\"Log-transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeJoinHist(train_test_selected,'Tau_PT_sum')\n",
    "plt.title(\"Original\")\n",
    "makeJoinHist(pd.concat([Y_train_test_selected,log(X_train_test_selected)],axis=1),'Tau_PT_sum')\n",
    "plt.title(\"Log-transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeJoinHist(train_test_selected,'Jet_Mass_sum')\n",
    "plt.title(\"Original\")\n",
    "makeJoinHist(pd.concat([Y_train_test_selected,log(X_train_test_selected)],axis=1),'Jet_Mass_sum')\n",
    "plt.title(\"Log-transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeJoinHist(train_test_selected,'Miss_MET')\n",
    "plt.title(\"Original\")\n",
    "makeJoinHist(pd.concat([Y_train_test_selected,log(X_train_test_selected)],axis=1),'Miss_MET')\n",
    "plt.title(\"Log-transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeJoinHist(train_test_selected,'Tau_Ener_sum')\n",
    "plt.title(\"Original\")\n",
    "makeJoinHist(pd.concat([Y_train_test_selected,log(X_train_test_selected)],axis=1),'Tau_Ener_sum')\n",
    "plt.title(\"Log-transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeJoinHist(train_test_selected,'Jet_Eta_dif')\n",
    "plt.title(\"Original\")\n",
    "makeJoinHist(pd.concat([Y_train_test_selected,log(X_train_test_selected)],axis=1),'Jet_Eta_dif')\n",
    "plt.title(\"Log-transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables have a significant improvement in the discrimination between the distributions of each **type** of event, except **Jet_Eta_dif**. Despite this, we would take the log transformation of the whole ($X$ variables) selected data for simplicity. At the end, this would not affect our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_log_train_test_selected = log(X_train_test_selected)\n",
    "X_log_train_test_selected.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Initialization\n",
    "\n",
    "The data will be splitted between train and test samples in a $1:1$ relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_log_train, X_log_test, Y_train, Y_test = train_test_split(X_log_train_test_selected,Y_train_test_selected,test_size=.5)\n",
    "X_log_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "To introduce the data in the model, it must be normalized. So after we split the data, the first and second momentum of the distributions must be:\n",
    "\n",
    "$$\\mu = 0$$ and $$\\sigma = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalizeMyDataFrameX(dataFrame,scaler):\n",
    "    scale_dataFrame = scaler.transform(dataFrame)\n",
    "    scale_dataFrame = pd.DataFrame(scale_dataFrame)\n",
    "    scale_dataFrame.columns = dataFrame.columns\n",
    "    return scale_dataFrame\n",
    "\n",
    "def normalizeMyDataFrameXY(dataFrame,scaler):\n",
    "    X_dataFrame = dataFrame.drop(['type'],axis=1) \n",
    "    X_scale_dataFrame = scaler.transform(X_dataFrame)\n",
    "    X_scale_dataFrame = pd.DataFrame(X_scale_dataFrame)\n",
    "    X_scale_dataFrame.columns = X_dataFrame.columns\n",
    "    return pd.concat([dataFrame['type'],X_scale_dataFrame],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Scaler from training data \n",
    "scaler = StandardScaler().fit(X_log_train)\n",
    "\n",
    "# Scale trainning set\n",
    "X_scale_log_train = normalizeMyDataFrameX(X_log_train,scaler)\n",
    "Y_train.reset_index(drop=True, inplace=True)\n",
    "scale_log_train = pd.concat([pd.DataFrame(Y_train),X_scale_log_train],axis=1)\n",
    "\n",
    "# Scale test set \n",
    "X_scale_log_test = normalizeMyDataFrameX(X_log_test,scaler)\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "scale_log_test = pd.concat([pd.DataFrame(Y_test),X_scale_log_test],axis=1)\n",
    "\n",
    "# Scale the validation test \n",
    "X_validation_selected = X_validation[['Jet_Ener_sum','Jet_PT_sum','Tau_PT_sum','Jet_Mass_sum','Miss_MET','Tau_Ener_sum','Jet_Eta_dif']]\n",
    "X_log_validation_selected = log(X_validation_selected)\n",
    "X_scale_log_validation = scaler.transform(X_log_validation_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "The first model to implement supposse a linear separation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(X_scale_log_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_logReg = logReg.predict(X_scale_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_TT = confusion_matrix(Y_test,Y_pred_logReg)\n",
    "confusion_matrix_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test,Y_pred_logReg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred_logReg_validation = logReg.predict(X_scale_log_validation)\n",
    "confusion_matrix_V = confusion_matrix(Y_validation,Y_pred_logReg_validation)\n",
    "confusion_matrix_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_validation,Y_pred_logReg_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network - MultiLayer Perceptron\n",
    "\n",
    "The second model with higher complexity incorporate some non-linearities that could have a better fitting of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(7,5,3),max_iter=500)\n",
    "clf.fit(X_scale_log_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_NN = clf.predict(X_scale_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix_TT = confusion_matrix(Y_test,Y_pred_NN)\n",
    "confusion_matrix_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,Y_pred_NN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred_NN_validation = clf.predict(X_scale_log_validation)\n",
    "confusion_matrix_V = confusion_matrix(Y_validation,Y_pred_NN_validation)\n",
    "confusion_matrix_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_validation,Y_pred_NN_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "The last model assumes a gaussian kernel to separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svmodel = svm.SVC(C=25,kernel = 'rbf')\n",
    "svmodel.fit(X_scale_log_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_SVM = svmodel.predict(X_scale_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix_TT = confusion_matrix(Y_test,Y_pred_SVM)\n",
    "confusion_matrix_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,Y_pred_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred_SVM_validation = svmodel.predict(X_scale_log_validation)\n",
    "confusion_matrix_V = confusion_matrix(Y_validation,Y_pred_SVM_validation)\n",
    "confusion_matrix_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_validation,Y_pred_SVM_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model\n",
    "\n",
    "Now, we want to choose the best model. Cause the three models get fairly the same high accuracy, the selection criteria will be to choose the simplest, i.e. Logistic regression. But, after we continue (just for fun), let's try to implement the model with the data that have the *log-transformation* but without normalitzation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression without normalized data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logRegWoN = LogisticRegression()\n",
    "logRegWoN.fit(X_log_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_logRegWoN = logRegWoN.predict(X_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix_TT = confusion_matrix(Y_test,Y_pred_logRegWoN)\n",
    "confusion_matrix_TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,Y_pred_logRegWoN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_logRegWoN_validation = logRegWoN.predict(X_log_validation_selected)\n",
    "confusion_matrix_V = confusion_matrix(Y_validation,Y_pred_logRegWoN_validation)\n",
    "confusion_matrix_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_validation,Y_pred_logRegWoN_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the *log-transformation* we could take the model without normalizing the data, and keep almost the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = logReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetExpectedNumberEvents(dataframe,model,N,cross_section,luminosity):\n",
    "    X_dataframe = dataframe.drop(['type'],axis=1)\n",
    "    Y_dataframe= dataframe['type']\n",
    "    \n",
    "    Y_pred = model.predict(X_dataframe)    \n",
    "    n = 1.0*sum(Y_pred)\n",
    "    N1 = Y_dataframe.count()\n",
    "    efficiency = n/N\n",
    "    N_event = efficiency*cross_section*luminosity\n",
    "\n",
    "    print \"From the initial N =\", N, \"events, it were selected N1 =\", N1, \"events with the C0 cuts\"\n",
    "    print \"From the N1 =\", N1, \"events, it were selected n =\", n, \"events by the MVA\"\n",
    "    print \"The final Efficiency of detection is : \", efficiency*100, \" %\"\n",
    "    print \"The expected number of events is: \", N_event\n",
    "    \n",
    "    return N_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import random sample from ROOT\n",
    "fullSignal = pd.read_csv(folder_full+signal_filename+'.csv')\n",
    "fullSignal.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullSignal['type'][fullSignal['Jet_Mass_sum']<=0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_signal= fullSignal[['type','Jet_Ener_sum','Jet_PT_sum','Tau_PT_sum','Jet_Mass_sum','Miss_MET','Tau_Ener_sum','Jet_Eta_dif']]\n",
    "full_signal_abs = full_signal.abs()\n",
    "full_signal_abs = full_signal_abs[full_signal_abs['Jet_Mass_sum']>0]\n",
    "full_signal_abs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide Data\n",
    "X_full_signal_abs = full_signal_abs.drop(['type'],axis=1)\n",
    "Y_full_signal_abs = full_signal_abs['type']\n",
    "# Scale Data\n",
    "scale_full_signal_abs = pd.concat([Y_full_signal_abs,normalizeMyDataFrameX(log(X_full_signal_abs),scaler)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "luminosity = 1000.0 # femtoBarns^-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_section_signal = 10.1123675492*1000 ## picoBarns\n",
    "N0_signal = 100000\n",
    "N_signal = GetExpectedNumberEvents(scale_full_signal_abs,final_model,N0_signal,cross_section_signal,luminosity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: W+jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import random sample from ROOT\n",
    "full_back1 = pd.read_csv(folder_full+'W+jets.root.csv')\n",
    "full_back1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_back1['type'][full_back1['Jet_Mass_sum']<=0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_back1 = full_back1[['type','Jet_Ener_sum','Jet_PT_sum','Tau_PT_sum','Jet_Mass_sum','Miss_MET','Tau_Ener_sum','Jet_Eta_dif']]\n",
    "full_back1_abs = full_back1.abs()\n",
    "full_back1_abs = full_back1_abs[full_back1_abs['Jet_Mass_sum']>0]\n",
    "full_back1_abs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide Data\n",
    "X_full_back1_abs = full_back1_abs.drop(['type'],axis=1)\n",
    "Y_full_back1_abs = full_back1_abs['type']\n",
    "# Scale Data\n",
    "scale_full_back1_abs = pd.concat([Y_full_back1_abs,normalizeMyDataFrameX(log(X_full_back1_abs),scaler)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_section_back1 = 32000000 # femtobarns\n",
    "N0_back1 = 43075659\n",
    "N_back1 = GetExpectedNumberEvents(scale_full_back1_abs,final_model,N0_back1,cross_section_back1,luminosity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: DY+jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import random sample from ROOT\n",
    "full_back2 = pd.read_csv(folder_full+'DY+jets.root.csv')\n",
    "full_back2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_back2['type'][full_back2['Jet_Mass_sum']<=0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_back2 = full_back2[['type','Jet_Ener_sum','Jet_PT_sum','Tau_PT_sum','Jet_Mass_sum','Miss_MET','Tau_Ener_sum','Jet_Eta_dif']]\n",
    "full_back2_abs = full_back2.abs()\n",
    "full_back2_abs = full_back2_abs[full_back2_abs['Jet_Mass_sum']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide Data\n",
    "X_full_back2_abs = full_back2_abs.drop(['type'],axis=1)\n",
    "Y_full_back2_abs = full_back2_abs['type']\n",
    "# Scale Data\n",
    "scale_full_back2_abs = pd.concat([Y_full_back2_abs,normalizeMyDataFrameX(log(X_full_back2_abs),scaler)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_section_back2 = 2240000 # femtoBarns\n",
    "N0_back2 = 29196813\n",
    "N_back2 = GetExpectedNumberEvents(scale_full_back2_abs,final_model,N0_back2,cross_section_back2,luminosity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: $t\\bar{t}$ production\n",
    "\n",
    "We can approximate the expected number of events for $t\\bar{t}$ production with $W+jets$ events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_back3 = N_back1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = (N_signal/10)/(N_signal+N_back1+N_back2+N_back3)**0.5\n",
    "print \"sigma : \", sigma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
